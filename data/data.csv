Title,Subtitle,Description,Images,Built With
RoBotany,we talk to plants but we not delusional,"One issue that we all seem to face is interpreting large sensor-based datasets. Whether it be financial or environmental data, we saw an opportunity to use LLMs to allow for better understanding of big data. As a proof of concept, taking care of a house plant or gardens was interesting because we could collect data and take actions based on physical metrics like soil moisture and sunlight. We were then inspired to take managing plants to the next level by being able to talk to the data you just collected in a fun way, like asking how each of your plants are doing. This is how RoBotany came to be.Through our web app, you can ask RoBotany about how your plants are doing - whether they need more water, have been overwatered, need to be in the shade, and many more questions. Each of your plants has a name, and you can ask specifically how your plant Jerry is faring for example. When you ask for a status update on your plants, our web app fetches data stored in our database, which gets a constant feed of information from the light and soil moisture sensors. If your plants are in need of water, you can even ask RoBotany to water your plants autonomously!HardwareThe hardware portion uses an Arduino, a photoresistor, and a soil moisture sensor to measure the quintessential environmental conditions of any indoor or outdoor plants. We even 3D-printed a flower pot specially made to hold a small plant and the Arduino system!FrontendOur frontend was built with React and uses the Chat UI Kit from chatscope. BackendOur project requires the use of two CockroachDBs. One of the databases is continuously read and updated for the soil moisture and light level, while the other database is updated less frequently to toggle the plant sprinkler system. Our simple yet practical endpoints allow us to seamlessly send information back and forth between our arduino and AWS EC2 instance, using technologies such as pm2 and nginx to keep the API up and running.NLPTo process user requests via our chatbot, we used a combination of a classification model on BentoML to categorize requests, as well as Cohere Generation for entity extraction and responding to more generic requests.The process goes as follows:One of the main challenges that we struggled with was working with LLMs, something none of our team was very familiar with. Despite being extremely challenging, we were glad we dove into the subject as deep as we did because it was equally rewarding to finally get it working.In addition, given that our electronic system was handling water, we wanted to make sure that our packaging protected our ESP32 and sensor boards. We started by designing a 3D printed compartment that would house everything from the electronics, to the Motor, to the plant itself. We quickly discovered a compartment that size would take well over 12 hours (we are at T-10 hours at that point). We modified our design to make it more compact, and were able to get a beautiful packaging done in time!Finally, from CockroachDB to Cohere, our group was managing a couple different authentication systems. Between refreshing tokens, as well as group members constantly hopping on and off different components, we ran into an issue quickly in terms of how to share the tokens. The solution - was to use Syroâ€™s secret management software.Our project had over a dozen unique technologies as our team looked to develop new skills and use new tech stacks during this hackathon.Some possible next steps include diversifying our plant sensor data, as well as making it more scalable, allowing users to potentially talk to an entire crop field!In addition, our system was designed with modularity in mind. Expanding to new, very different avenues of monitoring, shouldnâ€™t be complex tasks. RoBotany lays the groundwork for a smart platform to talk to variable sensor data.","['//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/591/793/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/591/792/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/592/105/datas/gallery.jpg']","['arduino', 'bentoml', 'cockroachdb', 'cohere', 'express.js', 'node.js', 'npm', 'psycopg2', 'python', 'react', 'sequelize', 'syro']"
DreamWeaver,A web app able to generate children's stories with text and image support.,"Home pagePrompt pageExample storyWe wanted to focus on education for children and knew the power of AI resources for both text and image generation. One of our members realized the lack of accessible picture books online and suggested that these stories could be generated instead. Our final idea was then DreamWeaverOur project uses a collection of machine learning resources to generate an entire children's book after being given any prompt. Our website is built upon the React framework, using MaterialUI as a core design library. Being able to generate relevant images that fit the theme and art style of the book was probably our biggest challenge and area to improve on. Trying to ensure a seamless flow and continuity in the generated imagery was a valuable learning experience and something we struggled with.  Another difficulty we faced was working with foreign APIs, as there were many times we struggled to understand how to access certain resources. Our use of machine learning, especially Cohere's LLM, is our project's most impressive aspect, as we combine different models and prompts to produce an entire book for the user. Our team worked extensively with AI APIs such as Cohere and Midjourney and learned a lot about how to use their APIs. Furthermore, we learned a lot about frontend development with React, as we spent a good amount of time designing the UI/UX of our web app. Having more continuity between book images would be the first thing to work on for our project, as sometimes different characters can be introduced by accident. We are also looking into adding support for multiple languages. ","['//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/597/302/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/597/303/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/994/datas/gallery.jpg']","['cohere', 'midjourney', 'react']"
Spyder,"If you can think it, you can drive it!","That underglow!Custom rack, pinion and motor mount for steeringElectrical wiring and custom PCBsCustom training setupsBrain waves!Fall lab and design bay cleanout leads to some pretty interesting things being put out at the free tables. In this case, we were drawn in by a motorized Audi Spyder car. And then, we saw the Neurosity Crown headsets, and an idea was born. A single late night call among team members, excited about the possibility of using a kiddy car for something bigger was all it took. Why can't we learn about cool tech and have fun while we're at it?Spyder is a way we can control cars with our minds. Use cases include remote rescue, non able-bodied individuals, warehouse, and being extremely cool.Spyder uses the Neurosity Crown to take the brainwaves of an individual, train an AI model to detect and identify certain brainwave patterns, and output them as a recognizable output to humans. It's a dry brain-computer interface (BCI) which means electrodes are placed against the scalp to read the brain's electrical activity. By taking advantage of these non-invasive method of reading electrical impulses, this allows for greater accessibility to neural technology.Collecting these impulses, we are then able to forward these commands to our Viam interface. Viam is a software platform that allows you to easily put together smart machines and robotic projects. It completely changed the way we coded this hackathon. We used it to integrate every single piece of hardware on the car. More about this below! :)The manual steering had to be converted to automatic. We did this in SolidWorks by creating a custom 3D printed rack and pinion steering mechanism with a motor mount that was mounted to the existing steering bracket. Custom gear sizing was used for the rack and pinion due to load-bearing constraints. This allows us to command it with a DC motor via Viam and turn the wheel of the car, while maintaining the aesthetics of the steering wheel.A 12V battery is connected to a custom soldered power distribution board. This powers the car, the boards, and the steering motor. For the DC motors, they are connected to a Cytron motor controller that supplies 10A to both the drive and steering motors via pulse-width modulation (PWM).A custom LED controller and buck converter PCB stepped down the voltage from 12V to 5V for the LED under glow lights and the Raspberry Pi 4. The Raspberry Pi 4 uses the Viam SDK (which controls all peripherals) and connects to the Neurosity Crown for vision software controlling for the motors. All the wiring is custom soldered, and many parts are custom to fit our needs.Viam was an integral part of our software development and hardware bringup. It significantly reduced the amount of code, testing, and general pain we'd normally go through creating smart machine or robotics projects. Viam was instrumental in debugging and testing to see if our system was even viable and to quickly check for bugs. The ability to test features without writing drivers or custom code saved us a lot of time. An exciting feature was how we could take code from Viam and merge it with a Go backend which is normally very difficult to do. Being able to integrate with Go was very cool - usually have to do python (flask + SDK). Being able to use Go, we get extra backend benefits without the headache of integration!Additional software that we used was python for the keyboard control client, testing, and validation of mechanical and electrical hardware. We also used JavaScript and node to access the Neurosity Crown, Neurosity SDK and Kinesis API to grab trained AI signals from the console. We then used websockets to port them over to the Raspberry Pi to be used in driving the car.Using the Neurosity Crown was the most challenging. Training the AI model to recognize a user's brainwaves and associate them with actions didn't always work. In addition, grabbing this data for more than one action per session was not possible which made controlling the car difficult as we couldn't fully realise our dream.Additionally, it only caught fire once - which we consider to be a personal best. If anything, we created the world's fastest smoke machine.We are proud of being able to complete a full mechatronics system within our 32 hours. We iterated through the engineering design process several times, pivoting multiple times to best suit our hardware availabilities and quickly making decisions to make sure we'd finish everything on time. It's a technically challenging project - diving into learning about neurotechnology and combining it with a new platform - Viam, to create something fun and useful.Cars are really cool! Turns out we can do more than we thought with a simple kid car. Viam is really cool! We learned through their workshop that we can easily attach peripherals to boards, use and train computer vision models, and even use SLAM! We spend so much time in class writing drivers, interfaces, and code for peripherals in robotics projects, but Viam has it covered. We were really excited to have had the chance to try it out!Neurotech is really cool! Being able to try out technology that normally isnâ€™t available or difficult to acquire and learn something completely new was a great experience.","['//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/299/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/300/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/551/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/707/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/708/datas/gallery.jpg']","['go', 'javascript', 'linux', 'node.js', 'python', 'raspberry-pi', 'viam']"
PearPiano,"AR composing tool, bringing music to life, not just to your ears but to your eyes ðŸŽ¶","A musician's best friend.Record your spontaneous compositions.Ask Pear any music-related questions!Interact with the AR piano.Drag-and-drop your notes for instant edits.Composing music through scribbling notes or drag-and-dropping from MuseScore couldn't be more tedious. As pianists ourselves, we know the struggle of trying to bring our impromptu improvisation sessions to life without forgetting what we just played or having to record ourselves and write out the notes one by one. Introducing PearPiano, a cute little pear that helps you pair the notes to your thoughts. As a musician's best friend, Pear guides pianists through an augmented simulation of a piano where played notes are directly translated into a recording and stored for future use. Pear can read both single notes and chords played on the virtual piano, allowing playback of your music with cascading tiles for full immersion. Seek musical guidance from Pear by asking, ""What is the key signature of C-major?"" or ""Tell me the notes of the E-major diminished 7th chord."" To fine tune your compositions, use ""Edit mode,"" where musicians can rewind the clip and drag-and-drop notes for instant changes.Using Unity Game Engine and the Oculus Quest, musicians can airplay their music on an augmented piano for real-time music composition. We used OpenAI's Whisper for voice dictation and C# for all game-development scripts. The AR environment is entirely designed and generated using the Unity UI Toolkit, allowing our engineers to realize an immersive yet functional musical corner.","['//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/588/511/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/496/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/497/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/498/datas/gallery.jpg', '//d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/590/495/datas/gallery.jpg']","['blender', 'c#', 'oculus-gear-vr', 'openai', 'unity', 'whisper']"
